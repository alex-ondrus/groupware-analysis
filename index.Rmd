---
title: "Practical Machine Learning - Groupware Analysis"
author: "Alexander Ondrus"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This exercise is to fulfill the final requirements of the Practical Machine Learning course from the Johns Hopkins University specialization *Data Science: Statistical Machine Learning*, available through Coursera. Using a provided training and testing set taken from data provided by:

> Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. *Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements*. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6.

Our aim is to predict the `classe` variable in the testing set with more than 80% accuracy.

## Load and Explore Data

I use the `lubridate` package to decompose the timestamp data into the month, day, day of the week, hour, and minute of each observation. I exclude the year as it has zero variance.

```{r load-explore, message=FALSE, warning=FALSE}
library(tidyverse)
library(magrittr)
library(caret)
library(lubridate)

training <- read_csv(
  "pml-training.csv",
  na = c("", "NA", "#DIV/0!"),
  guess_max = Inf,
  show_col_types = FALSE
  ) %>% 
  mutate(
    cvtd_timestamp = dmy_hm(cvtd_timestamp),
    TimeMonth = month(cvtd_timestamp),
    TimeDay = day(cvtd_timestamp),
    DayOfWeek = wday(cvtd_timestamp),
    TimeHour = hour(cvtd_timestamp),
    TimeMinute = minute(cvtd_timestamp)
  ) %>% 
  select(
    -`...1`, -cvtd_timestamp
  )

testing <- read_csv(
  "pml-testing.csv",
  na = c("", "NA", "#DIV/0!"),
  guess_max = Inf,
  show_col_types = FALSE
) %>% 
  mutate(
    cvtd_timestamp = dmy_hm(cvtd_timestamp),
    TimeMonth = month(cvtd_timestamp),
    TimeDay = day(cvtd_timestamp),
    DayOfWeek = wday(cvtd_timestamp),
    TimeHour = hour(cvtd_timestamp),
    TimeMinute = minute(cvtd_timestamp)
  ) %>% 
  select(
    -`...1`, -cvtd_timestamp
  )
```

### Variables Missing from Testing Data

There are 100 predictors that appear to be summary values of single repititions of the exercise. As none of the testing data has any of these summary variables included, I assume that the testing data are snapshots. From that, I deduce that our model needs to be able to predict only snapshots and so I exclude the summary predictors (and the `new_window` indicator, which takes a single value for the entire test set).

```{r strip-missing-values}
useless_columns <- testing %>% 
  select(
    where(~all(is.na(.)))
  ) %>% 
    colnames()

training %<>%
  select(-all_of(c(useless_columns, "new_window")))

testing %<>%
  select(-all_of(c(useless_columns, "new_window")))
```

### EDA

For exploratory data analysis, I use the `describe` function from the `Hmisc` package. Because the output is quite long, I've left it at the bottom of the page (in the section 'EDA Output'). Several of the values appear to contain outliers, and the classes we are predicting are imbalanced, but other than that there is nothing of specific interest.

### PCA Separability Test

To see if the classes are separable, I first apply PCA to the training set and examine whether the first two components provide any clear clustering of classes.

```{r pca-clusters}
training_pca <- prcomp(
  x = training %>% select(-classe, -user_name),
  scale. = TRUE
)

training_pca_plot <- training_pca$x %>% 
  as_tibble() %>% 
  mutate(classe = training$classe) %>% 
  ggplot(
    aes(
      x = PC1,
      y = PC2,
      colour = classe
    )
  ) + 
  geom_point(alpha = 0.2) +
  labs(
    title = "Principal Component Analysis of Predictor Variables",
    subtitle = "First two primary components"
  ) +
  theme_minimal()

plot(training_pca_plot)
```

While there is clearly clustering going on (and it doesn't appear that the clusters contain each class evenly), the first two components do not contain enough of the variation in the data to explain the cluster assignment.

## Model Selection

Given that there are a large number of quantitative variables and there is clustering in the data, I first tried various discriminant analysis techniques. Linear discriminant analysis was impaired by the co-linearity of the predictors. I attempted to overcome this by shifting to partial least squares discriminant analysis, but the accuracy was not what I had hoped. Flexible discriminant analysis performed better, but was still not optimum. 

Rather than attempt ensemble methods, I thought to try fitting a Neural Net. This produced impressive cross-validation accuracy results.

### Neural Net

First, I try a coarse parameter grid with 3-fold cross-validation parameter tuning to narrow my parameter search. The parameter search took hours to complete, so I comment out the code used and load in the completed model when knitting the page.

```{r nn-fit}
ctrl <- trainControl(
  method = "cv",
  number = 3,
  classProbs = TRUE,
  savePredictions = TRUE
)

# nnFit <- train(
#   classe ~ .,
#   data = training,
#   method = "nnet",
#   preProc = c("center", "scale", "spatialSign"),
#   tuneGrid = expand.grid(
#     .size = 1:10, .decay = 10^seq(from = -5, to = 0)
#   ),
#   trace = FALSE,
#   maxit = 2000,
#   trControl = ctrl
# )

nnFit <- read_rds("nnFitCoarse.rds")
```

It appears that the best parameters are `size = 7` and `decay = 0.001`.

```{r plot-nn-fit}
plot(nnFit)
```

To ensure that the variance in cross-validation accuracy estimate is not giving spurious results, I take the top 5 parameter sets and perform 10-fold cross-validation to compare them.

```{r fine-tuning}
best_5_params <- nnFit$results %>% 
  slice_max(
    order_by = Accuracy,
    n = 5
  ) %>% 
  select(
    size, decay
  )

ctrl <- trainControl(
  method = "cv",
  number = 10,
  classProbs = TRUE,
  savePredictions = TRUE
)

# nnFit_fine <- train(
#   classe ~ .,
#   data = training,
#   method = "nnet",
#   preProc = c("center", "scale", "spatialSign"),
#   tuneGrid = best_5_params,
#   trace = FALSE,
#   maxit = 2000,
#   trControl = ctrl
# )

nnFit_fine <- read_rds("nnFitFine.rds")
```

```{r plot-nn-fine}
plot(nnFit_fine)
```

We see that the the best fit parameters are `size = 10` and `decay = 0.001`.

## Generating Predictions

Now that I have trained the model, all that remains to do is to generate the predictions:

```{r gen-predict}
test_predictions <- predict(
  nnFit_fine,
  newdata = testing
)
```

### EDA Output

```{r eda-output, message=FALSE, warning=FALSE}
testing_describe_Hmisc <- Hmisc::describe(training)
Hmisc::html(testing_describe_Hmisc)
```

